#!/usr/bin/env python3
"""
COCO test2014 + RefCOCO / RefCOCO+ / RefCOCOg ä¸‹è½½å™¨ & è®­ç»ƒå­é›†æ„å»ºï¼ˆç²¾ç®€ç‰ˆï¼‰
- ä»…ä¾èµ–: datasets, pillow, tqdm
- å·²å­˜åœ¨æ–‡ä»¶è‡ªåŠ¨è·³è¿‡
- æ”¯æŒï¼šdownload / subset ä¸¤ä¸ªå­å‘½ä»¤ï¼ˆä¸å†™å­å‘½ä»¤é»˜è®¤ç­‰åŒäº downloadï¼‰

ç”¨æ³•ç¤ºä¾‹ï¼š
  # ä¸åŸè„šæœ¬å…¼å®¹çš„å¸¸è§ç”¨æ³•
  python coco_refcoco_cli_slim.py --datasets refcoco --root /content
  python coco_refcoco_cli_slim.py download -d coco refcoco refcocoplus refcocog --root /content
  python coco_refcoco_cli_slim.py download -d refcocog --splits val test
  python coco_refcoco_cli_slim.py subset --num-images 1000 --base-dir /content/refcoco \
      --ann-dataset-id jxu124/refcoco --coco-dataset-id visual-layer/coco-2014-vl-enriched \
      --subset-name train_1000 --one-bbox-policy random

å¯é€‰åŠ é€Ÿï¼š
  pip install -U hf_transfer  # ç„¶åä½¿ç”¨ --hf-transferï¼ˆé»˜è®¤å¼€å¯ï¼‰
"""
import os
import re
import sys
import json
import random
import argparse
from pathlib import Path
from typing import Callable, Dict, Iterable, Iterator, List, Optional

from PIL import Image
from tqdm.auto import tqdm
from datasets import load_dataset, get_dataset_split_names

# -------------------------------------------------
# å¸¸é‡
# -------------------------------------------------
REF_DATASETS = {
    "refcoco": "lmms-lab/RefCOCO",
    "refcocoplus": "lmms-lab/RefCOCOplus",
    "refcocog": "lmms-lab/RefCOCOg",
}
DEFAULT_REF_SPLITS_ORDER = ["val", "testA", "testB", "test"]

# -------------------------------------------------
# å·¥å…·
# -------------------------------------------------

def _json_safe(v):
    """å°½é‡æŠŠ HF çš„ç±»å‹è½¬æˆ JSON å¯åºåˆ—åŒ–ã€‚"""
    try:
        json.dumps(v)
        return v
    except TypeError:
        if isinstance(v, (set,)):
            return list(v)
        if isinstance(v, (bytes, bytearray)):
            return v.decode("utf-8", errors="ignore")
        return str(v)


def save_annotations_jsonl(ds, out_path: Path, drop_image_col: str = "image", overwrite: bool = False) -> None:
    """æŠŠæ•°æ®é›†å»æ‰ image åˆ—åå†™æˆ JSONLï¼ˆé€è¡Œï¼‰ï¼Œé¿å…å¤§æ–‡ä»¶ã€‚"""
    out_path.parent.mkdir(parents=True, exist_ok=True)
    if out_path.exists() and out_path.stat().st_size > 0 and not overwrite:
        print(f"âœ… æ ‡æ³¨å·²å­˜åœ¨ï¼š{out_path}")
        return

    cols = [c for c in ds.column_names if c != drop_image_col]
    print(f"âœï¸  ä¿å­˜æ ‡æ³¨ â†’ {out_path}")
    with open(out_path, "w", encoding="utf-8") as f:
        for ex in ds.select_columns(cols):
            f.write(json.dumps({k: _json_safe(v) for k, v in ex.items()}, ensure_ascii=False) + "\n")
    print(f"âœ… æ ‡æ³¨å·²ä¿å­˜ï¼š{out_path}")


def save_annotations_jsonl_iter(ds_iter: Iterable[dict], out_path: Path, drop_image_col: str = "image", overwrite: bool = False, limit: Optional[int] = None) -> None:
    """Write annotations from an iterable dataset (e.g., streaming) to JSONL, dropping the image column."""
    out_path.parent.mkdir(parents=True, exist_ok=True)
    if out_path.exists() and out_path.stat().st_size > 0 and not overwrite:
        print(f"â„¹ï¸  æ³¨é‡Šå·²å­˜åœ¨ï¼š{out_path}")
        return

    written = 0
    with open(out_path, "w", encoding="utf-8") as f:
        for idx, ex in enumerate(ds_iter):
            if limit is not None and idx >= limit:
                break
            rec = {k: _json_safe(v) for k, v in ex.items() if k != drop_image_col}
            f.write(json.dumps(rec, ensure_ascii=False) + "\n")
            written += 1
    print(f"? æ³¨é‡Šå·²ä¿å­˜ï¼š{out_path}ï¼ˆ{written} è¡Œï¼‰")


def _default_name(ex: Dict, idx: int, prefix: str = "img_") -> str:
    name = ex.get("file_name")
    if not name:
        name = f"{prefix}{idx:06d}.jpg"
    if "." not in Path(name).name:
        name = f"{name}.jpg"
    return Path(name).name  # é˜²æ­¢è·¯å¾„ç©¿è¶Š


def save_images(
    ds_iter: Iterable[dict],
    out_dir: Path,
    *,
    name_fn: Optional[Callable[[dict, int], str]] = None,
    image_col: str = "image",
    overwrite: bool = False,
    total: Optional[int] = None,
    limit: Optional[int] = None,
) -> None:
    """æŠŠå¯è¿­ä»£çš„æ ·æœ¬ï¼ˆå« PIL å›¾åƒï¼‰ä¿å­˜åˆ°ç£ç›˜ã€‚æ”¯æŒ streaming å’Œé streamingã€‚"""
    out_dir.mkdir(parents=True, exist_ok=True)

    # å°è¯•è‡ªåŠ¨è®¡ç®—æ€»æ•°ï¼ˆé streamingï¼‰
    if total is None:
        try:
            total = len(ds_iter)  # type: ignore[arg-type]
        except Exception:
            pass

    written = skipped = 0
    with tqdm(total=(limit if limit is not None else total), desc=f"ä¿å­˜å›¾ç‰‡ â†’ {out_dir.name}") as bar:
        for idx, ex in enumerate(ds_iter):
            if limit is not None and idx >= limit:
                break
            img: Image.Image = ex[image_col]
            fname = (name_fn or _default_name)(ex, idx)
            out_path = out_dir / fname
            out_path.parent.mkdir(parents=True, exist_ok=True)

            if out_path.exists() and not overwrite:
                skipped += 1
                bar.update(1)
                continue
            try:
                img.save(out_path, format="JPEG")
                written += 1
            except Exception as e:
                print(f"âš ï¸  ä¿å­˜å¤±è´¥ {out_path}: {e}")
            bar.update(1)

    print(f"âœ… å›¾ç‰‡å®Œæˆï¼šå†™å…¥ {written}ï¼Œè·³è¿‡ {skipped}ï¼Œç›®æ ‡ç›®å½• {out_dir}")


# -------------------------------------------------
# ä¸‹è½½ï¼šCOCO test2014
# -------------------------------------------------

def download_coco_test(out_images_dir: Path, out_ann_dir: Path, *, overwrite: bool = False, limit: Optional[int] = None) -> None:
    print("==== visual-layer/coco-2014-vl-enriched :: test ====")
    ds = load_dataset("visual-layer/coco-2014-vl-enriched", split="test")

    def coco_test_name(ex: dict, _i: int) -> str:
        iid = ex.get("image_id")
        if iid is None:
            return f"COCO_test2014_{_i:06d}.jpg"
        return f"COCO_test2014_{int(iid):012d}.jpg"  # 12 ä½ zero-padï¼Œæ›´æ ‡å‡†

    save_images(ds, out_images_dir, name_fn=coco_test_name, image_col="image", overwrite=overwrite, total=len(ds), limit=limit)
    save_annotations_jsonl(ds, out_ann_dir / "test.jsonl", drop_image_col="image", overwrite=overwrite)


# -------------------------------------------------
# ä¸‹è½½ï¼šRefCOCO / RefCOCO+ / RefCOCOg
# -------------------------------------------------

def download_ref_like(dataset_key: str, base_dir: Path, *, splits: Optional[List[str]] = None, overwrite: bool = False, limit: Optional[int] = None) -> None:
    dataset_id = REF_DATASETS[dataset_key]
    available = get_dataset_split_names(dataset_id)
    target_splits = [sp for sp in (splits or DEFAULT_REF_SPLITS_ORDER) if sp in available]

    if not target_splits:
        print(f"âš ï¸  æœªå‘ç°å¯ç”¨åˆ‡åˆ†ï¼ˆ{dataset_id}ï¼‰")
        return

    for sp in target_splits:
        print(f"==== {dataset_id} :: {sp} ====")
        ds_stream_for_images = load_dataset(dataset_id, split=sp, streaming=True)
        out_images = base_dir / sp
        save_images(
            ds_stream_for_images,
            out_images,
            name_fn=lambda ex, i: _default_name(ex, i),
            image_col="image",
            overwrite=overwrite,
            limit=limit,
        )
        ds_stream_for_ann = load_dataset(dataset_id, split=sp, streaming=True)
        save_annotations_jsonl_iter(ds_stream_for_ann, base_dir / f"{sp}.jsonl", drop_image_col="image", overwrite=overwrite, limit=limit)


# -------------------------------------------------
# æ„å»º RefCOCO è®­ç»ƒå­é›†ï¼ˆæµå¼ä¸‹è½½ COCO train2014ï¼‰
# -------------------------------------------------

def _pad12(x) -> str:
    s = re.sub(r"\D", "", str(x))
    return f"{int(s):012d}"


def _id12_from_fname(fn: str) -> Optional[str]:
    m = re.search(r"([0-9]{12})", str(fn))
    return m.group(1) if m else None


def build_refcoco_train_subset(
    *,
    num_images: int = 1000,
    seed: int = 42,
    ann_dataset_id: str = "jxu124/refcoco",
    coco_dataset_id: str = "visual-layer/coco-2014-vl-enriched",
    base_dir: Path = Path("refcoco"),
    subset_name: str = "train_1000",
    one_bbox_policy: str = "random",  # random | largest | first
    overwrite: bool = False,
) -> None:
    print(f"==== æ„å»º RefCOCO è®­ç»ƒå­é›†ï¼š{num_images} å¼ å”¯ä¸€å›¾ç‰‡ï¼Œæ¯å›¾ 1 æ¡æ ‡æ³¨ ====")

    # 1) è®­ç»ƒæ ‡æ³¨ï¼ˆä¸å«å›¾åƒï¼‰
    ann = load_dataset(ann_dataset_id, split="train")

    # image_id â†’ indices
    if "image_id" in ann.column_names:
        ids12 = [_pad12(x) for x in ann["image_id"]]
    elif "file_name" in ann.column_names:
        ids12 = [_id12_from_fname(fn) for fn in ann["file_name"]]
    else:
        raise ValueError(f"{ann_dataset_id} ç¼ºå°‘ image_id/file_name")

    by_id: Dict[str, List[int]] = {}
    for i, iid in enumerate(ids12):
        if iid is None:
            continue
        by_id.setdefault(iid, []).append(i)

    uniq_ids = list(by_id.keys())
    if num_images > len(uniq_ids):
        print(f"âš ï¸  ç›®æ ‡ {num_images} > å¯ç”¨å”¯ä¸€å›¾ç‰‡ {len(uniq_ids)}ï¼Œè‡ªåŠ¨è°ƒå°ã€‚")
        num_images = len(uniq_ids)

    rng = random.Random(seed)
    chosen_ids = rng.sample(uniq_ids, k=num_images)

    def pick_one(idxs: List[int]) -> int:
        if one_bbox_policy == "largest" and "bbox" in ann.column_names:
            return max(idxs, key=lambda j: (ann[j]["bbox"][2] * ann[j]["bbox"][3]) if ann[j]["bbox"] else -1.0)
        if one_bbox_policy == "first":
            return idxs[0]
        return rng.choice(idxs)

    chosen_indices = [pick_one(by_id[iid]) for iid in chosen_ids]
    ann_subset = ann.select(chosen_indices)

    # 2) ä¿å­˜å­é›†æ ‡æ³¨
    out_images_dir = base_dir / subset_name
    save_annotations_jsonl(ann_subset, base_dir / f"{subset_name}.jsonl", drop_image_col="image", overwrite=overwrite)

    # 3) ä»…ä¸‹è½½å‘½ä¸­çš„ COCO train2014 å›¾ç‰‡ï¼ˆstreamingï¼‰
    coco_stream = load_dataset(coco_dataset_id, split="train", streaming=True)
    chosen_set = set(chosen_ids)

    def iter_matching() -> Iterator[dict]:
        remaining = set(chosen_set)
        for ex in coco_stream:
            iid12 = f"{int(ex['image_id']):012d}"
            if iid12 in remaining:
                if not ex.get("file_name"):
                    ex["file_name"] = f"COCO_train2014_{iid12}.jpg"
                remaining.remove(iid12)
                yield ex
            if not remaining:
                break

    save_images(
        iter_matching(),
        out_images_dir,
        name_fn=lambda ex, i: Path(ex.get("file_name") or f"COCO_train2014_{int(ex['image_id']):012d}.jpg").name,
        image_col="image",
        overwrite=overwrite,
        total=len(chosen_ids),
    )

    print(f"âœ… å­é›†å®Œæˆï¼š{len(chosen_ids)} å¼ å”¯ä¸€å›¾ç‰‡ï¼Œ{len(ann_subset)} æ¡æ ‡æ³¨ï¼ˆæ¯å›¾ 1 æ¡ï¼‰")
    print(f"   - å›¾ç‰‡ â†’ {out_images_dir}")
    print(f"   - æ ‡æ³¨ â†’ {base_dir / (subset_name + '.jsonl')}")


# -------------------------------------------------
# CLI
# -------------------------------------------------

def build_parser() -> argparse.ArgumentParser:
    p = argparse.ArgumentParser(
        description="COCO test2014 + RefCOCO/+/g ä¸‹è½½ & è®­ç»ƒå­é›†æ„å»ºï¼ˆç²¾ç®€ CLIï¼‰",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )
    p.add_argument("--root", type=Path, default=Path("."), help="è¾“å‡ºæ ¹ç›®å½•")
    p.add_argument("--hf-transfer", dest="hf_transfer", action="store_true", help="å¼€å¯ hf_transfer ä¼ è¾“åŠ é€Ÿï¼ˆè‹¥å·²å®‰è£…ï¼‰")
    p.add_argument("--no-hf-transfer", dest="hf_transfer", action="store_false", help="å…³é—­ hf_transfer ä¼ è¾“åŠ é€Ÿ")
    p.set_defaults(hf_transfer=True)

    # download test
    p.add_argument("--mode", choices=["test", "train"], default="test", help="ä¸‹è½½æ¨¡å¼ï¼štestï¼ˆä¸‹è½½ COCO test2014 + Ref*ï¼‰æˆ– trainï¼ˆæ„å»º RefCOCO è®­ç»ƒå­é›†ï¼‰")
    p.add_argument("--datasets", "-d", nargs="+", choices=["coco", "refcoco", "refcocoplus", "refcocog"], default=["refcoco"], help="é€‰æ‹©è¦ä¸‹è½½çš„æ•°æ®é›†")
    p.add_argument("--coco-dir", type=Path, default=None, help="COCO test2014 å›¾ç‰‡è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ root/data/coco_images/test2014ï¼‰")
    p.add_argument("--refcoco-dir", type=Path, default=None, help="RefCOCO è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ root/data/refcocoï¼‰")
    p.add_argument("--refcocoplus-dir", type=Path, default=None, help="RefCOCO+ è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ root/data/refcocoplusï¼‰")
    p.add_argument("--refcocog-dir", type=Path, default=None, help="RefCOCOg è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ root/data/refcocogï¼‰")
    p.add_argument("--splits", nargs="+", choices=["val", "testA", "testB", "test"], help="Ref* æ•°æ®é›†çš„ç›®æ ‡åˆ‡åˆ†")
    p.add_argument("--overwrite", action="store_true", help="è¦†ç›–åŒåæ–‡ä»¶/æ ‡æ³¨")
    p.add_argument("--limit", type=int, default=None, help="ä»…ä¸‹è½½å‰ N å¼ å›¾ç‰‡ï¼ˆè°ƒè¯•ç”¨ï¼‰")

    # train subset
    p.add_argument("--num-images", type=int, default=1000, help="å”¯ä¸€å›¾ç‰‡æ•°é‡ï¼ˆæ¯å›¾ 1 æ¡æ ‡æ³¨ï¼‰")
    p.add_argument("--seed", type=int, default=42, help="éšæœºç§å­")
    p.add_argument("--ann-dataset-id", type=str, default="jxu124/refcoco", help="è®­ç»ƒæ ‡æ³¨æ•°æ®é›† IDï¼ˆä¸å«å›¾åƒï¼‰")
    p.add_argument("--coco-dataset-id", type=str, default="visual-layer/coco-2014-vl-enriched", help="COCO 2014 æ•°æ®é›† IDï¼ˆå«å›¾åƒï¼‰")
    p.add_argument("--base-dir", type=Path, default=None, help="è¾“å‡ºåŸºç›®å½•ï¼ˆé»˜è®¤ root/refcocoï¼‰")
    p.add_argument("--subset-name", type=str, default="train_1000", help="å­é›†åç§°ï¼ˆå›¾ç‰‡ç›®å½•å & æ ‡æ³¨æ–‡ä»¶åå‰ç¼€ï¼‰")
    p.add_argument("--one-bbox-policy", choices=["random", "largest", "first"], default="random", help="æ¯å›¾é€‰æ‹©å“ªä¸€æ¡æ ‡æ³¨")

    return p


def resolve_dirs(args):
    root: Path = args.root
    coco_dir = getattr(args, "coco_dir", None) or (root / "data" / "coco_images" / "test2014")
    refcoco_dir = getattr(args, "refcoco_dir", None) or (root / "data" / "refcoco")
    refcocoplus_dir = getattr(args, "refcocoplus_dir", None) or (root / "data" / "refcocoplus")
    refcocog_dir = getattr(args, "refcocog_dir", None) or (root / "data" / "refcocog")
    for p in [coco_dir, refcoco_dir, refcocoplus_dir, refcocog_dir]:
        p.mkdir(parents=True, exist_ok=True)
    return coco_dir, refcoco_dir, refcocoplus_dir, refcocog_dir


def main() -> None:

    parser = build_parser()
    args = parser.parse_args()

    # hf_transfer ç¯å¢ƒå˜é‡ï¼ˆæœªå®‰è£…ä¸ä¼šæŠ¥é”™ï¼‰
    os.environ["HF_HUB_ENABLE_HF_TRANSFER"] = "1" if args.hf_transfer else "0"

    mode = args.mode

    if mode == "test":
        coco_dir, refcoco_dir, refcocoplus_dir, refcocog_dir = resolve_dirs(args)

        if "coco" in args.datasets:
            download_coco_test(
                out_images_dir=coco_dir,
                out_ann_dir=coco_dir,
                overwrite=args.overwrite,
                limit=args.limit,
            )

        if "refcoco" in args.datasets:
            download_ref_like("refcoco", base_dir=refcoco_dir, splits=args.splits, overwrite=args.overwrite, limit=args.limit)
        if "refcocoplus" in args.datasets:
            download_ref_like("refcocoplus", base_dir=refcocoplus_dir, splits=args.splits, overwrite=args.overwrite, limit=args.limit)
        if "refcocog" in args.datasets:
            download_ref_like("refcocog", base_dir=refcocog_dir, splits=args.splits, overwrite=args.overwrite, limit=args.limit)

        print("ğŸ‰ å…¨éƒ¨å®Œæˆï¼š")
        print(f"- COCO test2014 å›¾ç‰‡ â†’ {coco_dir}")
        print(f"- RefCOCO å›¾ç‰‡/æ ‡æ³¨ â†’ {refcoco_dir}")
        print(f"- RefCOCO+ å›¾ç‰‡/æ ‡æ³¨ â†’ {refcocoplus_dir}")
        print(f"- RefCOCOg å›¾ç‰‡/æ ‡æ³¨ â†’ {refcocog_dir}")

    elif mode == "train":
        # åŸºç›®å½•é»˜è®¤ä¸ä¸‹è½½æ—¶ä¸€è‡´ï¼ˆroot/refcocoï¼‰
        _, refcoco_dir, _, _ = resolve_dirs(argparse.Namespace(root=args.root, coco_dir=None, refcoco_dir=None, refcocoplus_dir=None, refcocog_dir=None))
        base_dir = args.base_dir or refcoco_dir
        base_dir.mkdir(parents=True, exist_ok=True)

        build_refcoco_train_subset(
            num_images=args.num_images,
            seed=args.seed,
            ann_dataset_id=args.ann_dataset_id,
            coco_dataset_id=args.coco_dataset_id,
            base_dir=base_dir,
            subset_name=args.subset_name,
            one_bbox_policy=args.one_bbox_policy,
            overwrite=args.overwrite,
        )

    else:
        parser.print_help()


if __name__ == "__main__":
    main()
